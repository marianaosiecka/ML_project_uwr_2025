{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8fz9kt5_zLS"
      },
      "source": [
        "# Online Shoppers Purchasing Intention\n",
        "### *Machine Learning Project*, UWr 2024/2025\n",
        "\n",
        "*   Denys Tsebulia 351322,\n",
        "*   Mafalda Costa 351255,\n",
        "*   Mariana Carvalho 351254."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Table of Contents**\n",
        "- [1. Introduction](#1-introduction)\n",
        "   - [1.1. Required libraries](#imports)\n",
        "   - [1.2. Metrics](#12-metrics)\n",
        "   - [1.3. Attributes](#13-attributes)\n",
        "- [2. Data analysis](#2-data-analysis)\n",
        "- [3. Data pre-processing](#3-data-pre-processing)\n",
        "- [4. Classification](#4-classification)\n",
        "   - [Decision Tree](#41-decision-tree)\n",
        "   - [Random Forest](#42-random-forest)\n",
        "   - [Support Vector Machines (SVM)](#44-support-vector-machines-svm)\n",
        "   - [XGBoost](#46-xgboost)\n",
        "- [5. Results analysis](#5-results-analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **1. Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W9htu37_2Bm"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "_UabrIg2pQT8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV,KFold, RandomizedSearchCV, ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "from sklearn.neighbors import KNeighborsClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i2nWmKB-ct4"
      },
      "source": [
        "## **2. Data analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "GXPYSL7Pop3D",
        "outputId": "0fcdf296-030a-400a-92a5-071f8f628316"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"online_shoppers_intention.csv\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "rfoyI7WPpXY6",
        "outputId": "ccdd9960-73b2-4303-8c0a-8bf22267fb6a"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9jMh1RdpiO8",
        "outputId": "b84d08a8-20d6-4d80-cf8c-de6840536d32"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHLnHy32f9hK"
      },
      "source": [
        "As it can be seen, the majority of the data consists of numerical features. However, there are 8 categorical features: 'Month', 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType', 'Weekend' and 'Revenue'. To utilize these features in our models, we will need to convert them into numerical representations.\n",
        "\n",
        "To **convert 'Revenue', the target feature, into a numerical representation**, we can update the values using the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V4l2N3te-LQ-"
      },
      "outputs": [],
      "source": [
        "data[\"Revenue\"] = data[\"Revenue\"].apply(lambda x: 1 if x == True else 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUe1aFi_-nj3"
      },
      "source": [
        "### **Separating target, categorical and numerical features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vbjA5ZLS-OF_"
      },
      "outputs": [],
      "source": [
        "target=\"Revenue\"\n",
        "categorical_features=[\"OperatingSystems\", \"Browser\", \"Region\", \"TrafficType\", \"VisitorType\", \"Weekend\", \"Month\"]\n",
        "numerical_features=data.columns.drop(categorical_features).drop(target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0X69PKy-uqR"
      },
      "source": [
        "### **Distribution of 'Revenue'**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9ZFUIyv-wSL"
      },
      "source": [
        "To visualize the distribution of the target feature 'Revenue', we use a count plot. This plot displays the number of occurrences for each class in the 'Revenue' feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "6HGr_feD-sQQ",
        "outputId": "bb677651-d6eb-4ce4-cc32-bc5d41f4cdd4"
      },
      "outputs": [],
      "source": [
        "sb.countplot(x=target,data=data)\n",
        "plt.title(\"Revenue\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5u5z9HF-8Tx"
      },
      "source": [
        "We can observe an imbalance between the two classes of the target feature. The number of instances labeled as 0 ('False') is significantly larger than the number of instances labeled as 1 ('True').\n",
        "This can potentially affect the performance of certain machine learning models, and appropriate techniques such as defining class weights, oversampling and undersampling are required to address this issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8KEpjsJ--Vo"
      },
      "source": [
        "### **NULL values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "66DnvEfk-x1N",
        "outputId": "c8f0e285-c6a4-4ead-ffd7-78d7349f7e06"
      },
      "outputs": [],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH3vU23i_FIk"
      },
      "source": [
        "### **Correlation Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AS2PHa3_G8a"
      },
      "source": [
        "To explore the relationships between all the numerical features, we generate a heatmap:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 995
        },
        "id": "u8yfj2l__Elq",
        "outputId": "ce642436-ca1a-49e1-e3f7-e8fa50029400"
      },
      "outputs": [],
      "source": [
        "temp_data = numerical_features.append(pd.Index([target])) # created Index object to match the numerical features\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "sb.heatmap(data[temp_data].corr(), annot=True, cmap=\"RdBu_r\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IapPXgq_Mp0"
      },
      "source": [
        "Based on the heatmap, the analysis yields the following observations:\n",
        "- There is **generally very little correlation** among the features.\n",
        "- There are a few cases of **high correlation** (|corr| >= 0.7):\n",
        "    - BounceRates & ExitRates (0.9).\n",
        "    - ProductRelated & ProductRelated_Duration (0.86).\n",
        "- There are **moderate correlations** (0.3 < |corr| < 0.7):\n",
        "    - Among the following features: Administrative, Administrative_Duration, Informational, Informational_Duration, ProductRelated, and ProductRelated_Duration.\n",
        "    - And between PageValues and Revenue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsSAyxDE_OfO"
      },
      "source": [
        "To further visualize the relationships between the features, we generate a pairplot for each pair of features, where each session is represented through a purchase (in orange) or no purchase (blue):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YmtjPzwK_A7o",
        "outputId": "748087c3-9329-4bab-cd47-cd49407b2fdf"
      },
      "outputs": [],
      "source": [
        "sb.pairplot(data, hue='Revenue', vars=numerical_features.append(pd.Index([target])), corner=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhfeZock_qUu"
      },
      "source": [
        "From the pairplot, the following observations can be made:\n",
        "- There is no strong correlation between 'Revenue' and any other feature.\n",
        "- Notably, there is a strong negative correlation between PageValues and other features shown in the plot.\n",
        "- We can also see that the there are a few outliers present in the data that may need to be addressed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GvkW8yXHcRo"
      },
      "source": [
        "### **Duplicate rows**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VUizAtq_P69",
        "outputId": "1964fd73-ee7f-462a-9717-58552f9af254"
      },
      "outputs": [],
      "source": [
        "print(\"Total number of duplicate rows: \", data.duplicated().sum())\n",
        "\n",
        "duplicates = data[data.duplicated()]\n",
        "print(\"Duplicate rows:\")\n",
        "print(duplicates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF8ZF_pg_sLD"
      },
      "source": [
        "## **3. Data pre-processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSpYy9Zz_t8N"
      },
      "source": [
        "### **Remove duplicate rows**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqU67lDLpHWw",
        "outputId": "3719fa7d-6a88-402a-eff4-47454a487128"
      },
      "outputs": [],
      "source": [
        "data.drop_duplicates(inplace=True)\n",
        "print(\"Total number of duplicate rows: \", data.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BoaWqEx_y56"
      },
      "source": [
        "### **Outliers analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqoTLoBS_0EH"
      },
      "source": [
        "Outliers are anomalies within the dataset and are rare occurrences.\n",
        "In this problem, we can think, for example, of a situation where the user accidentally leaves their desktop open on a product page, only to return half an hour later. The website's analytics would register this as the user spending half an hour only reading the product page - a highly unlikely scenario. Such events are not representative of typical user behavior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahv1Vxn3_1ZA"
      },
      "source": [
        "To analyise the outliers, we can look at the pairplot above and the generated boxplots (for each numerical feature, grouped by the 'Revenue' target variable) below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5BM8ywXS_xF_",
        "outputId": "90a06bbd-c865-4748-f8a3-1740e59821bd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i, collumn in enumerate(numerical_features):\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    sb.boxplot(data=data, x=target, y=collumn)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "fhrv1Fi4v-AZ",
        "outputId": "88f136ff-f517-4cdd-dc4f-ef1add3296f5"
      },
      "outputs": [],
      "source": [
        "'''import matplotlib.cbook as cbook\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i, column in enumerate(numerical_features):\n",
        "    stats = {}\n",
        "    stats = cbook.boxplot_stats(data[column], labels=[column])[0]\n",
        "\n",
        "    # set the 2nd and 98th percentiles as the box edges\n",
        "    stats['q1'], stats['q3'] = np.percentile(data[column], [2, 98])\n",
        "    stats['iqr'] = stats['q3'] - stats['q1']  # update IQR\n",
        "\n",
        "    # recompute whiskers\n",
        "    outlier_threshold = 1.5\n",
        "    stats['whislo'] = np.min(data[column][data[column] >= stats['q1'] - outlier_threshold * stats['iqr']])\n",
        "    stats['whishi'] = np.max(data[column][data[column] <= stats['q3'] + outlier_threshold * stats['iqr']])\n",
        "\n",
        "    # update outliers\n",
        "    stats['fliers'] = data[column][(data[column] < stats['whislo']) | (data[column] > stats['whishi'])].tolist()\n",
        "\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.bxp([stats], showfliers=True)\n",
        "    ax.set_title(column)\n",
        "\n",
        "plt.show() '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iP07J0RADql"
      },
      "source": [
        "We can observe that there are outliers present in all features, for both 'Revenue' categories, and that the median and IQR values really vary within the same feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHHjAElRAEya"
      },
      "source": [
        "To remove the outliers, we used the calculation Interquartille Range (IQR) by calculating the percentile for each of the features. We have decided to cut off only very \"far out\" information from the dataset thus including majority of the data, which is between the 2nd\n",
        "percentile and 98th percentile. This is because there is a risk of losing important information if too much data is removed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JT7fxXDu__8A",
        "outputId": "bd7a6b4f-d6c8-4cd7-a462-24833fd6bbf8"
      },
      "outputs": [],
      "source": [
        "Q1 = data[numerical_features].quantile(0.02)\n",
        "Q3 = data[numerical_features].quantile(0.98)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "outlier_threshold = 1.5 # standard threshold\n",
        "\n",
        "# identify outliers for each feature --> create a boolean data frame where a 'True' indicates an outlier in the corresponding feature and row\n",
        "outliers = (data[numerical_features] < (Q1 - outlier_threshold * IQR)) | (data[numerical_features] > (Q3 + outlier_threshold * IQR))\n",
        "\n",
        "# remove outliers from the dataset --> only rows where no features are flagged as outliers are kept\n",
        "data_no_outliers = data[numerical_features][~(outliers.any(axis=1))]\n",
        "\n",
        "# print the number of removed outliers\n",
        "num_outliers_removed = len(data) - len(data_no_outliers)\n",
        "print(\"Number of outliers removed:\", num_outliers_removed)\n",
        "\n",
        "# replace the original data with the no outliers data\n",
        "data[numerical_features] = data_no_outliers\n",
        "\n",
        "# drop rows with missing values --> removing rows with outliers of the numerical columns will leave gaps (NaN) in the other columns\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "for i, collumn in enumerate(numerical_features):\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    sb.boxplot(data=data, x=target, y=collumn)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aEEkC0WAYpz"
      },
      "source": [
        "### **Encode categorical features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwJ_hDZtLKyd"
      },
      "source": [
        "We first deal with the boolean feature 'Weekend', by transforming its value to 1 ('True') and 0 ('False')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-Yb5nqZ0K0CO"
      },
      "outputs": [],
      "source": [
        "data[\"Weekend\"] = data[\"Weekend\"].apply(lambda x: 1 if x == True else 0)\n",
        "categorical_features.remove(\"Weekend\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZtQ-sZsAbtn"
      },
      "source": [
        "To handle the rest of the categorical features in our dataset, we use OneHotEnconder. This transforms the categorical features into numerical representations, and, as a result, the 8 categorical features are expanded into 66 attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "5eTRmyxzAKUi",
        "outputId": "43d714d9-2fc3-42a4-911e-0a6204cafaae"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "for feature in categorical_features:\n",
        "    onehotarray = encoder.fit_transform(data[[feature]]).toarray()\n",
        "    items = [f'{feature}_{item}' for item in encoder.categories_[0]]\n",
        "    data[items] = onehotarray\n",
        "\n",
        "data=data.drop(categorical_features, axis=1)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnyRCjZQK-5D"
      },
      "source": [
        "### **Scaler**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnGll17IAm97"
      },
      "source": [
        "Firstly, we exclude the target feature from the set of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9lYmadvAdDJ",
        "outputId": "3b74ae72-a07b-4a7d-cc3e-113ae708f53c"
      },
      "outputs": [],
      "source": [
        "features=list(data.columns)\n",
        "features.remove('Revenue')\n",
        "features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNgN0ouOAr7l"
      },
      "source": [
        "We apply feature scaling to our subsets, using MinMaxScaler. Feature scaling is particularly beneficial for models that are sensitive to the magnitude of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "-Zek5lcCAoan",
        "outputId": "e561a3db-9f17-40ec-fe13-21ecc298db60"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# (X - X_min) / (X_max - X_min)\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data)\n",
        "scaled_data = pd.DataFrame(scaler.transform(data), columns=data.columns)\n",
        "scaled_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq8CtmDZA-0z"
      },
      "source": [
        "### **Feature selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBRihVr2BAYw"
      },
      "source": [
        "In order to optimize our model's performance and reduce unnecessary complexity, we performed feature selection. With a total of 75 input features, it was essential to identify the features that have the most significant impact on the 'Revenue' and remove those that have a negligible effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRI_CSCzBBoJ"
      },
      "source": [
        "This function performs feature selection using the chi-squared test as the scoring function. It selects the top 'n' features based on their scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG_IYWPEA1Gb",
        "outputId": "1205425d-9cda-4d58-9f18-921748d57525"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "#chi2 (from wikipedia):\n",
        "#In simpler terms, this test is primarily used to examine\n",
        "#whether two categorical variables are independent in influencing the test statistic .\n",
        "\n",
        "def feature_selection(features, n, data):\n",
        "    selector = SelectKBest(score_func=chi2, k=n)\n",
        "    fit=selector.fit(data[features],data[target])\n",
        "\n",
        "    selected_feature_indices = selector.get_support(indices=True)\n",
        "\n",
        "    selected_features = [data[features].columns[i] for i in selected_feature_indices]\n",
        "    return selected_features\n",
        "\n",
        "feature_dict_data = {}\n",
        "for i in range(2, int(len(features))):\n",
        "    feature_dict_data[i] = feature_selection(features, i, data)\n",
        "\n",
        "for key in list(feature_dict_data.keys())[:5]:\n",
        "    print(f\"Key: {key}, Value: {feature_dict_data[key]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNZiXHaeAxYk"
      },
      "source": [
        "### **Resampling and class weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sKnfUTVTI3e",
        "outputId": "544ceec0-abf2-45a0-8af1-0bd9ccfc4c66"
      },
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "rbQkTDVUDJgW",
        "outputId": "39ed4591-b17b-4c3e-92eb-0db927f87e39"
      },
      "outputs": [],
      "source": [
        "data[target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzwLEr1DTL6G"
      },
      "outputs": [],
      "source": [
        "#implement weighted-based resampling\n",
        "class_counts = np.bincount(data[target])\n",
        "class_weights = len(data) / (len(class_counts) * class_counts)\n",
        "weights_train = np.array([class_weights[label] for label in data[target]])\n",
        "weights_train_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "print(\"Class counts:\", class_counts)\n",
        "print(\"Class weights:\", class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**SMOTEENN** applies **SMOTE** (oversampling) to the minority class, adding synthetic samples to the data which creates a more balanced set. Then **EditedNearestNeighbours** is used to \"clean\" the majority class, since new observations from the minority class were added we need to pay attention to keeping the samples from each class distinct. \n",
        "- How are the synthetic samples generated? SMOTE identifies the minority class data points and selects k nearest neighbors for each point (default is 5). A synthetic data point is then created by randomly sampling from the feature space between the minority class data point and one of its k nearest neighbors. \n",
        "- How are the samples from the majority class \"cleaned\"? By removing samples close to the decision boundary. Observations from the majority class are removed when any or most of its closest neighours (default is 3) are from a different class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "smote_enn = SMOTEENN(sampling_strategy=0.7, enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
        "resampled_data, resampled_target = smote_enn.fit_resample(data[features], data[target])\n",
        "resampled_data[target] = resampled_target\n",
        "\n",
        "# print the number of samples in each class after resampling\n",
        "print(\"Class counts after resampling:\", np.bincount(resampled_target))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zqMXl3vqPRM"
      },
      "source": [
        "## **4. Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40WL0X2Fqa_7"
      },
      "source": [
        "### **K-Fold and evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jk_ttsUwVQZ8"
      },
      "outputs": [],
      "source": [
        "def training_and_predictions_with_KFold(Algorithm, name,num_features, data=data, draw_confusion_matrix=False):\n",
        "    kf =KFold(n_splits=10)\n",
        "    predictions = []\n",
        "    results = []\n",
        "    prediction_probs = []  # To store prediction probabilities\n",
        "    selected_features=feature_dict_data[num_features]\n",
        "    all_inputs = data[selected_features].values\n",
        "\n",
        "    all_labels = data[target].values\n",
        "\n",
        "    for i, (train_index, test_index) in enumerate(kf.split(all_inputs, all_labels)):\n",
        "        # Split the data into training and testing sets\n",
        "        X_train, X_test = all_inputs[train_index], all_inputs[test_index]\n",
        "        y_train, y_test = all_labels[train_index], all_labels[test_index]\n",
        "        Algorithm.fit(X_train, y_train)\n",
        "        predictions.append(Algorithm.predict(X_test))\n",
        "        results.append(y_test)\n",
        "        if(hasattr(Algorithm, \"predict_proba\")):\n",
        "            prediction_probs.append(Algorithm.predict_proba(X_test)[:, 1])\n",
        "\n",
        "    all_predictions = np.concatenate(predictions)\n",
        "    all_results = np.concatenate(results)\n",
        "    if(hasattr(Algorithm, \"predict_proba\")):\n",
        "        all_prediction_probs = np.concatenate(prediction_probs)\n",
        "\n",
        "    #calculates the metrics\n",
        "    accuracy = accuracy_score(all_results, all_predictions)\n",
        "    precision = precision_score(all_results, all_predictions)\n",
        "    recall = recall_score(all_results, all_predictions)\n",
        "    f1 = f1_score(all_results, all_predictions)\n",
        "\n",
        "    #prints the metrics, confusion matrix and ROC curve\n",
        "    if draw_confusion_matrix:\n",
        "        df = pd.DataFrame({\"Model\"           : [name],\n",
        "                       \"Accuracy_score\"  : [accuracy],\n",
        "                       \"Recall_score\"    : [recall],\n",
        "                       \"Precision_score\"       : [precision],\n",
        "                       \"F1_score\"        : [f1],\n",
        "                      })\n",
        "        conf_matrix = confusion_matrix(all_results, all_predictions)\n",
        "        plt.figure(figsize=(10, 5))  # Increase figure size for better visibility\n",
        "        plt.subplot(1, 2, 1)  # First subplot for confusion matrix\n",
        "        sb.heatmap(conf_matrix, cmap='Blues', annot=True, xticklabels=data['Revenue'].unique(), yticklabels=data['Revenue'].unique())\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.title(\"Confusion matrix for \"+name)\n",
        "\n",
        "        # ROC curve\n",
        "        plt.subplot(1, 2, 2)  # Second subplot for ROC curve\n",
        "        fpr, tpr, thresholds = roc_curve(all_results, all_prediction_probs)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('ROC for '+name)\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.tight_layout()  # Adjust subplot parameters to give specified padding\n",
        "        plt.show()\n",
        "        \n",
        "        # Precision Recall curve\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        precision, recall, thresholds = precision_recall_curve(all_results, all_prediction_probs)\n",
        "        precision_recall_auc = auc(recall, precision)\n",
        "        plt.plot(recall, precision, label='Precision Recall curve (area = %0.2f)' % precision_recall_auc)\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title('Precision Recall curve for '+name)\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "\n",
        "        # Return both DataFrame and ROC curve metrics\n",
        "        return df, {'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc}, {'precision': precision, 'recall': recall, 'precision_recall_auc': precision_recall_auc}\n",
        "    return accuracy, precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SZR7BvLzWBl5"
      },
      "outputs": [],
      "source": [
        "def features_num_training_and_predictions_with_KFold(Algorithm, name, data=data):\n",
        "    accuracy_scores=[]\n",
        "    precision_scores=[]\n",
        "    recall_scores=[]\n",
        "    f1_scores=[]\n",
        "    size_features=[]\n",
        "    max_f1=0\n",
        "    max_precision=0\n",
        "    num_features=0\n",
        "    for i in range(2, int(len(features))):\n",
        "        accuracy_score, precision_score, recall_score, f1_score=training_and_predictions_with_KFold(Algorithm, name, i, data, False)\n",
        "        accuracy_scores.append(accuracy_score)\n",
        "        precision_scores.append(precision_score)\n",
        "        recall_scores.append(recall_score)\n",
        "        f1_scores.append(f1_score)\n",
        "        size_features.append(i)\n",
        "        if(f1_score>max_f1):\n",
        "            max_f1=f1_score\n",
        "            max_precision=precision_score\n",
        "            num_features=i\n",
        "        if(f1_score==max_f1 and precision_score>max_precision):\n",
        "            max_precision=precision_score\n",
        "            num_features=i\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(size_features, accuracy_scores, label=\"accuracy\")\n",
        "    plt.plot(size_features, precision_scores, label=\"precision\")\n",
        "    plt.plot(size_features, recall_scores, label=\"recall\")\n",
        "    plt.plot(size_features, f1_scores, label=\"f1\")\n",
        "    plt.xlabel(\"Number of features\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.title(\"Scores for different number of features of \"+name)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(f\"Features chosen: {feature_dict_data[num_features]}\")\n",
        "    return num_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "o614cPc3WEXC"
      },
      "outputs": [],
      "source": [
        "roc_curve_data = {}\n",
        "roc_curve_data_resampled = {}\n",
        "\n",
        "pr_curve_data = {}\n",
        "pr_curve_data_resampled = {}\n",
        "\n",
        "results_training = {}\n",
        "results_training_resampled = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "cross_validation = KFold(n_splits=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_classifier = RandomForestClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Grid search** is a hyperparameter optimization technique used to find the best combination of hyperparameters for a model. It systematically goes through multiple combinations of parameter values, cross-validating as it goes to determine which combination provides the best performance.\n",
        "\n",
        "Use grid search to find the best parameters for Random Forest using f1-score as the scoring function (with the original data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''random_forest_parameter_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'n_estimators': [10,100],\n",
        "    'max_depth': [ 4, 5, 6, 9, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
        "    'max_features': ['log2', 2, 4],\n",
        "    'max_leaf_nodes': [None, 2,  5],\n",
        "    'min_samples_leaf': [ 2, 4,],\n",
        "    'min_samples_split': [2],\n",
        "    'class_weight': ['balanced', 'balanced_subsample', None, weights_train_dict],\n",
        "    'random_state': [100],\n",
        "    'ccp_alpha': [ 0.1, 0.2,0.4, 0.5]\n",
        "}\n",
        "\n",
        "random_forest_grid_search = GridSearchCV( random_forest_classifier, param_grid=random_forest_parameter_grid, cv=cross_validation, scoring='f1')\n",
        "random_forest_grid_search.fit(data[features].values, data[target].values)\n",
        "\n",
        "print('Best score for Random Forest: {}'.format(random_forest_grid_search.best_score_))\n",
        "print('Best parameters for Random Forest: {}'.format(random_forest_grid_search.best_params_))'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best score for Random Forest: 0.6717258371942657\n",
        "\n",
        "Best parameters for Random Forest: {'ccp_alpha': 0.1, 'class_weight': {0: 0.5914115230362907, 1: 3.2348849652220437}, 'criterion': 'entropy', 'max_depth': 5, 'max_features': 'log2', 'max_leaf_nodes': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 100}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply grid search to find the best parameters for Random Forest **with the resampled data**, using accuracy as the scoring function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''random_forest_parameter_grid_resampled = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'n_estimators': [10,100],\n",
        "    'max_depth': [ 4, 5, 6, 9, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
        "    'max_features': ['log2', 2, 4],\n",
        "    'max_leaf_nodes': [None, 2,  5],\n",
        "    'min_samples_leaf': [ 2, 4,],\n",
        "    'min_samples_split': [2],\n",
        "    'random_state': [100],\n",
        "    'ccp_alpha': [ 0.1, 0.2,0.4, 0.5]\n",
        "}\n",
        "\n",
        "random_forest_grid_search_resampled = GridSearchCV( random_forest_classifier, param_grid=random_forest_parameter_grid_resampled, cv=cross_validation, scoring='accuracy') \n",
        "random_forest_grid_search_resampled.fit(resampled_data[features].values, resampled_data[target].values)\n",
        "\n",
        "print('Best score for Random Forest with Resampled Data: {}'.format(random_forest_grid_search_resampled.best_score_))\n",
        "print('Best parameters for Random Forest with Resampled Data: {}'.format(random_forest_grid_search_resampled.best_params_))'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best score for Random Forest with Resampled Data: 0.8327955678797455\n",
        "\n",
        "Best parameters for Random Forest with Resampled Data: {'ccp_alpha': 0.1, 'criterion': 'entropy', 'max_depth': 4, 'max_features': 'log2', 'max_leaf_nodes': 5, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 100}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''# random forest classifier\n",
        "random_forest_classifier = random_forest_grid_search.best_estimator_\n",
        "\n",
        "# random forest classifier with resampled data\n",
        "random_forest_classifier_resampled = random_forest_grid_search_resampled.best_estimator_'''\n",
        "\n",
        "random_forest_classifier = RandomForestClassifier(ccp_alpha=0.1, class_weight=weights_train_dict, criterion='entropy', max_depth=5, max_features='log2', max_leaf_nodes=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100, random_state=100)\n",
        "\n",
        "random_forest_classifier_resampled = RandomForestClassifier(ccp_alpha=0.1, criterion='entropy', max_depth=4, max_features='log2', max_leaf_nodes=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100, random_state=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''num_features_random_forest=features_num_training_and_predictions_with_KFold(random_forest_classifier, \"Random Forest\", data=data)\n",
        "print(\"Number of features Random Forest: \", num_features_random_forest)\n",
        "\n",
        "num_features_random_forest_resampled=features_num_training_and_predictions_with_KFold(random_forest_classifier_resampled, \"Random Forest with Resampled Data\", resampled_data)\n",
        "print(\"Number of features Random Forest with Resampled Data: \", num_features_random_forest_resampled)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Number of features Random Forest:  12\n",
        "\n",
        "Number of features Random Forest with Resampled Data:  4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_features_random_forest = 12\n",
        "num_features_random_forest_resampled = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# random forest\n",
        "# get evaluation metrics and ROC curve\n",
        "model_RF_Kfold, roc_curve_RF, pr_curve_RF = training_and_predictions_with_KFold(random_forest_classifier, \"Random Forest\", num_features_random_forest, data, draw_confusion_matrix=True)\n",
        "\n",
        "# random forest with resampled data\n",
        "model_RF_Kfold_resampled, roc_curve_RF_resampled, pr_curve_RF_resampled = training_and_predictions_with_KFold(random_forest_classifier_resampled, \"Random Forest with Resampled Data\", num_features_random_forest_resampled, resampled_data, draw_confusion_matrix=True)\n",
        "\n",
        "# plot confusion matrix and ROC curve\n",
        "roc_curve_data[\"Random Forest\"] = roc_curve_RF\n",
        "roc_curve_data_resampled[\"Random Forest Resampling\"] = roc_curve_RF_resampled\n",
        "\n",
        "pr_curve_data[\"Random Forest\"] = pr_curve_RF\n",
        "pr_curve_data_resampled[\"Random Forest Resampling\"] = pr_curve_RF_resampled\n",
        "\n",
        "results_training[\"Random Forest\"] = model_RF_Kfold\n",
        "results_training_resampled[\"Random Forest Resampling\"] = model_RF_Kfold_resampled\n",
        "\n",
        "# table with model performances\n",
        "model_performances = pd.concat([model_RF_Kfold, model_RF_Kfold_resampled],axis = 0).reset_index()\n",
        "model_performances = model_performances.drop(columns = \"index\",axis =1) \n",
        "model_performances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "decision_tree_classifier = DecisionTreeClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''parameter_grid_decision_tree = {\n",
        "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'min_samples_leaf': [3, 4, 5, 6], \n",
        "    'max_leaf_nodes': [None, 2, 3],\n",
        "    'max_features': [None, 'sqrt', 'log2', 2, 4], \n",
        "    'min_samples_split': [2, 3, 4],\n",
        "    'class_weight': [None, 'balanced', weights_train_dict],\n",
        "    'random_state': [None, 42, 100],\n",
        "    'ccp_alpha': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "      }\n",
        "\n",
        "grid_search_decision_tree = GridSearchCV( decision_tree_classifier,\n",
        "                        param_grid=parameter_grid_decision_tree,\n",
        "                        cv=cross_validation,\n",
        "                        scoring='f1')\n",
        "grid_search_decision_tree.fit(data[features].values, data[target].values)\n",
        "\n",
        "print('Best score of Decision Tree: {}'.format(grid_search_decision_tree.best_score_))\n",
        "print('Best parameters of Decision Tree: {}'.format(grid_search_decision_tree.best_params_))'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best score of Decision Tree: 0.6713675895396494\n",
        "\n",
        "Best parameters of Decision Tree: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': 2, 'min_samples_leaf': 3, 'min_samples_split': 2, 'random_state': None}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''parameter_grid_decision_tree_resampled = {\n",
        "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'min_samples_leaf': [3, 4, 5, 6], \n",
        "    'max_leaf_nodes': [None, 2, 3],\n",
        "    'max_features': [None, 'sqrt', 'log2', 2, 4], \n",
        "    'min_samples_split': [2, 3, 4],\n",
        "    'random_state': [None, 42, 100],\n",
        "    'ccp_alpha': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "}\n",
        "\n",
        "grid_search_decision_tree_resampled = GridSearchCV( decision_tree_classifier,\n",
        "                        param_grid=parameter_grid_decision_tree_resampled,\n",
        "                        cv=cross_validation,\n",
        "                        scoring='accuracy')\n",
        "\n",
        "grid_search_decision_tree_resampled.fit(resampled_data[features].values, resampled_data[target].values)\n",
        "\n",
        "print('Best score of Decision Tree with Resampled Data: {}'.format(grid_search_decision_tree_resampled.best_score_))\n",
        "print('Best parameters of Decision Tree with Resampled Data: {}'.format(grid_search_decision_tree_resampled.best_params_))'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best score of Decision Tree with Resampled Data: 0.9265847228214594\n",
        "\n",
        "Best parameters of Decision Tree with Resampled Data: {'ccp_alpha': 0.0, 'criterion': 'entropy', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_samples_leaf': 3, 'min_samples_split': 2, 'random_state': None}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''decision_tree_classifier = grid_search_decision_tree.best_estimator_\n",
        "decision_tree_classifier_resampled = grid_search_decision_tree_resampled.best_estimator_'''\n",
        "\n",
        "decision_tree_classifier = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy', max_depth=3, max_features=None, max_leaf_nodes=2, min_samples_leaf=3, min_samples_split=2, random_state=None)\n",
        "decision_tree_classifier_resampled = DecisionTreeClassifier(ccp_alpha=0.0, criterion='entropy', max_depth=3, max_features=None, max_leaf_nodes=None, min_samples_leaf=3, min_samples_split=2, random_state=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''# decision tree\n",
        "num_features_decision_tree = features_num_training_and_predictions_with_KFold(decision_tree_classifier, \"Decision Tree\", data)\n",
        "print(\"Number of features for Decision Tree: \", num_features_decision_tree)\n",
        "\n",
        "# decision tree with class weights\n",
        "num_features_decision_tree_resampled=features_num_training_and_predictions_with_KFold(decision_tree_classifier_resampled, \"Decision Tree with Resampled Data\", resampled_data)\n",
        "print(\"Number of features for Decision Tree with Resampled Data: \", num_features_decision_tree_resampled)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Number of features for Decision Tree:  2\n",
        "\n",
        "Number of features for Decision Tree with Resampled Data:  8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_features_decision_tree = 2\n",
        "num_features_decision_tree_resampled = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# decision tree\n",
        "# get evaluation metrics\n",
        "model_DT_Kfold, roc_curve_DT, pr_curve_DT = training_and_predictions_with_KFold(decision_tree_classifier, \"Decision Tree\", num_features_decision_tree, data, draw_confusion_matrix=True)\n",
        "\n",
        "# decision tree with class weights\n",
        "model_DT_Kfold_resampled, roc_curve_DT_resampled, pr_curve_DT_resampled = training_and_predictions_with_KFold(decision_tree_classifier_resampled, \"Decision Tree with Resampled Data\", num_features_decision_tree_resampled, resampled_data, draw_confusion_matrix=True)\n",
        "\n",
        "# plot confusion matrix and ROC curve\n",
        "roc_curve_data[\"Decision Tree\"] = roc_curve_DT\n",
        "roc_curve_data_resampled[\"Decision Tree with Resampled Data\"] = roc_curve_DT_resampled\n",
        "\n",
        "pr_curve_data[\"Decision Tree\"] = pr_curve_DT\n",
        "pr_curve_data_resampled[\"Decision Tree with Resampled Data\"] = pr_curve_DT_resampled\n",
        "\n",
        "results_training[\"Decision Tree\"] = model_DT_Kfold\n",
        "results_training_resampled[\"Decision Tree with Resampled Data\"] = model_DT_Kfold_resampled\n",
        "\n",
        "# table with model performances\n",
        "model_performances = pd.concat([model_DT_Kfold, model_DT_Kfold_resampled],axis = 0).reset_index()\n",
        "model_performances = model_performances.drop(columns = \"index\",axis =1)\n",
        "model_performances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler.fit(resampled_data)\n",
        "scaled_resampled_data = pd.DataFrame(scaler.transform(resampled_data), columns=resampled_data.columns)\n",
        "scaled_resampled_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb = XGBClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scale_pos_weight = round(np.bincount(data[target])[0] / np.bincount(data[target])[1])\n",
        "\n",
        "print(f\"scale_pos_weight parameter: {scale_pos_weight}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_parameter_grid = {'n_estimators': [50],\n",
        "                    'max_depth': [3, 5, 10, 50, 100],\n",
        "                    'learning_rate': [0.1],\n",
        "                    'reg_alpha': [0.5],\n",
        "                    'reg_lambda': [0, 0.2, 0.5, 0.8],\n",
        "                    'booster': ['gbtree'],\n",
        "                    'objective': ['binary:logistic'],\n",
        "                    'subsample': [0.5, 0.7, 1.0],\n",
        "                    'colsample_bytree': [0.5, 1.0],\n",
        "                    'gamma': [0, 1],\n",
        "                    'scale_pos_weight' : [1, scale_pos_weight]  #default value is 1, for imbalanced data typical value is ratio of majority class to minority class\n",
        "}\n",
        "\n",
        "#xgb_grid_search = GridSearchCV(xgb, param_grid=xgb_parameter_grid, cv=cross_validation, scoring='f1')\n",
        "\n",
        "#xgb_grid_search.fit(scaled_data[features].values, scaled_data[target].values)\n",
        "\n",
        "#print('Best score for XGBoost: {}'.format(xgb_grid_search.best_score_))\n",
        "#print('Best parameters for XGBoost: {}'.format(xgb_grid_search.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best score for XGBoost: 0.6875894526582997\n",
        "\n",
        "Best parameters for XGBoost: {'booster': 'gbtree', 'colsample_bytree': 1.0, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 50, 'objective': 'binary:logistic', 'reg_alpha': 0.5, 'reg_lambda': 0.8, 'scale_pos_weight': 5, 'subsample': 0.5}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_parameter_grid_resampled = {'n_estimators': [50],\n",
        "                    'max_depth': [3, 5, 10, 50, 100],\n",
        "                    'learning_rate': [0.1],\n",
        "                    'reg_alpha': [0.5],\n",
        "                    'reg_lambda': [0, 0.2, 0.5, 0.8],\n",
        "                    'booster': ['gbtree'],\n",
        "                    'objective': ['binary:logistic'],\n",
        "                    'subsample': [0.5, 0.7, 1.0],\n",
        "                    'colsample_bytree': [0.5, 1.0],\n",
        "                    'gamma': [0, 1],\n",
        "}\n",
        "\n",
        "'''xgb_resampled_grid_search = GridSearchCV(xgb, param_grid=xgb_parameter_grid_resampled, cv=cross_validation, scoring='accuracy')\n",
        "\n",
        "xgb_resampled_grid_search.fit(scaled_resampled_data[features].values, scaled_resampled_data[target].values)\n",
        "\n",
        "print('Best score for XGBoost with Resampled Data: {}'.format(xgb_resampled_grid_search.best_score_))\n",
        "print('Best parameters for XGBoost with Resampled Data: {}'.format(xgb_resampled_grid_search.best_params_))'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best score for XGBoost with Resampled Data: 0.9264455101054334\n",
        "\n",
        "Best parameters for XGBoost with Resampled Data: {'booster': 'gbtree', 'colsample_bytree': 1.0, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'objective': 'binary:logistic', 'reg_alpha': 0.5, 'reg_lambda': 0.8, 'subsample': 1.0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''# xgboost classifier\n",
        "xgb_classifier = xgb_grid_search.best_estimator_\n",
        "\n",
        "# xgboost classifier for resampled data\n",
        "xgb_classifier_resampled = xgb_resampled_grid_search.best_estimator_'''\n",
        "\n",
        "xgb_classifier = XGBClassifier(booster='gbtree', colsample_bytree=1.0, gamma=1, learning_rate=0.1, max_depth=10, n_estimators=50, objective= 'binary:logistic', reg_alpha=0.5, reg_lambda=0.8, scale_pos_weight=5, subsample=0.5)\n",
        "\n",
        "xgb_classifier_resampled = XGBClassifier(booster='gbtree', colsample_bytree=1.0, gamma=1, learning_rate=0.1, max_depth=3, n_estimators=50, objective= 'binary:logistic', reg_alpha=0.5, reg_lambda=0.8, subsample=1.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best number of features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''# retrieving the best number of features for this classifier\n",
        "num_features_xgboost =features_num_training_and_predictions_with_KFold(xgb_classifier, \"XGBoost\", scaled_data)\n",
        "print(\"Number of features for XGBoost: \", num_features_xgboost)\n",
        "\n",
        "num_features_xgboost_resample=features_num_training_and_predictions_with_KFold(xgb_classifier_resampled, \"XGBoost Resampling\", scaled_resampled_data)\n",
        "print(\"Number of features for XGBoost with the resampled data: \", num_features_xgboost_resample)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Number of features for XGBoost:  48"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Number of features for XGBoost with the resampled data:  12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_features_xgboost = 48\n",
        "num_features_xgboost_resample = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performance metrics: confusion matrix, ROC curve, accuracy, recall, precision and F1 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# xgboost\n",
        "# get evaluation metrics and ROC curve\n",
        "xgb_Kfold, roc_curve_XGB, pr_curve_XGB =training_and_predictions_with_KFold(xgb_classifier, \"XGBoost\", num_features_xgboost, scaled_data, draw_confusion_matrix=True)\n",
        "\n",
        "# xgboost for resampled data\n",
        "xgb_Kfold_resampling, roc_curve_XGB_resampling, pr_curve_XGB_resampled = training_and_predictions_with_KFold(xgb_classifier_resampled, \"XGBoost Resampling\", num_features_xgboost_resample, scaled_resampled_data, draw_confusion_matrix=True)\n",
        "\n",
        "# plot confusion matrix and ROC curve\n",
        "roc_curve_data[\"XGBoost\"] = roc_curve_XGB\n",
        "roc_curve_data_resampled[\"XGBoost Resampling\"] = roc_curve_XGB_resampling\n",
        "\n",
        "pr_curve_data[\"XGBoost\"] = pr_curve_XGB\n",
        "pr_curve_data_resampled[\"XGBoost Resampling\"] = pr_curve_XGB_resampled\n",
        "\n",
        "results_training[\"XGBoost\"] = xgb_Kfold\n",
        "results_training_resampled[\"XGBoost Resampling\"] = xgb_Kfold_resampling\n",
        "\n",
        "# table with model performances\n",
        "model_performances = pd.concat([xgb_Kfold, xgb_Kfold_resampling],axis = 0).reset_index()\n",
        "model_performances = model_performances.drop(columns = \"index\",axis =1) \n",
        "model_performances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "knn = KNeighborsClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parameter_grid_knn = {\n",
        "    'n_neighbors': list(range(1, 5)),\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'algorithm': ['kd_tree', 'auto'],\n",
        "    'n_jobs': [1,2],\n",
        "    'leaf_size': [ 30, 40],\n",
        "    'p': [1, 2, 3],\n",
        "    'metric': ['manhattan'],\n",
        "}\n",
        "\n",
        "'''grid_search_knn = GridSearchCV( knn,\n",
        "                           param_grid=parameter_grid_knn,\n",
        "                           cv=cross_validation,\n",
        "                           scoring='f1')\n",
        "\n",
        "grid_search_knn.fit(data[features].values, data[target].values)\n",
        "\n",
        "print('Best score: {}'.format(grid_search_knn.best_score_))\n",
        "print('Best parameters: {}'.format(grid_search_knn.best_params_))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parameter_grid_knn_resampled = {\n",
        "    'n_neighbors': list(range(1, 5)),\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'algorithm': ['kd_tree', 'auto'],\n",
        "    'n_jobs': [1,2],\n",
        "    'leaf_size': [ 30, 40],\n",
        "    'p': [1, 2, 3],\n",
        "    'metric': ['manhattan'],\n",
        "}\n",
        "\n",
        "'''grid_search_knn_resampled = GridSearchCV( knn,\n",
        "                           param_grid=parameter_grid_knn_resampled,\n",
        "                           cv=cross_validation,\n",
        "                           scoring = 'accuracy')\n",
        "\n",
        "grid_search_knn_resampled.fit(resampled_data[features].values, resampled_data[target].values)\n",
        "\n",
        "print('Best score: {}'.format(grid_search_knn_resampled.best_score_))\n",
        "print('Best parameters: {}'.format(grid_search_knn_resampled.best_params_))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''knn_classifier = grid_search_knn.best_estimator_\n",
        "knn_classifier_resampled = grid_search_knn_resampled.best_estimator_'''\n",
        "\n",
        "#Best parameters: {'algorithm': 'kd_tree', 'leaf_size': 30, 'metric': 'manhattan', 'n_jobs': 1, 'n_neighbors': 4, 'p': 1, 'weights': 'distance'}\n",
        "knn_classifier = KNeighborsClassifier(algorithm='kd_tree', leaf_size=30, metric='manhattan', n_jobs=1, n_neighbors=4, p=1, weights='distance')\n",
        "knn_classifier_resampled = KNeighborsClassifier(algorithm='kd_tree', leaf_size=30, metric='manhattan', n_jobs=1, n_neighbors=1, p=1, weights='uniform')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''num_features_knn = features_num_training_and_predictions_with_KFold(knn_classifier, \"KNN\", data)\n",
        "print(\"Number of features for KNN: \", num_features_knn)\n",
        "\n",
        "num_features_knn_resampled = features_num_training_and_predictions_with_KFold(knn_classifier_resampled, \"KNN Resampling\", resampled_data)''''''\n",
        "print(\"Number of features for KNN with the resampled data: \", num_features_knn_resampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Number of features for KNN:  2\n",
        "\n",
        "Number of features for KNN with the resampled data:  65"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_features_knn = 2\n",
        "num_features_knn_resampled = 65"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# knn\n",
        "# get evaluation metrics ROC curve and Precision Recall Curve\n",
        "knn_Kfold, roc_curve_KNN, pr_curve_KNN = training_and_predictions_with_KFold(knn_classifier, \"KNN\", num_features_knn, data, draw_confusion_matrix=True)\n",
        "\n",
        "# knn for resampled data\n",
        "knn_Kfold_resampling, roc_curve_KNN_resampling, pr_curve_KNN_resampled = training_and_predictions_with_KFold(knn_classifier_resampled, \"KNN Resampling\", num_features_knn_resampled, resampled_data, draw_confusion_matrix=True)\n",
        "\n",
        "# plot confusion matrix and ROC curve\n",
        "roc_curve_data[\"KNN\"] = roc_curve_KNN\n",
        "roc_curve_data_resampled[\"KNN Resampling\"] = roc_curve_KNN_resampling\n",
        "\n",
        "pr_curve_data[\"KNN\"] = pr_curve_KNN\n",
        "pr_curve_data_resampled[\"KNN Resampling\"] = pr_curve_KNN_resampled\n",
        "\n",
        "results_training[\"KNN\"] = knn_Kfold\n",
        "results_training_resampled[\"KNN Resampling\"] = knn_Kfold_resampling\n",
        "\n",
        "# table with model performances\n",
        "model_performances = pd.concat([knn_Kfold, knn_Kfold_resampling],axis = 0).reset_index()\n",
        "model_performances = model_performances.drop(columns = \"index\",axis =1) \n",
        "model_performances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "#svm = SVC()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''parameter_grid_svm = {'C': [10, 100, 1000],\n",
        "                  'kernel': ['poly', 'rbf'],\n",
        "                  'gamma': [1, 0.1,0.01],\n",
        "                  'coef0': [0.0, 0.1, 0.2],\n",
        "                  'class_weight': [None, 'balanced', weights_train_dict],\n",
        "                  }\n",
        "\n",
        "grid_search_svm = RandomizedSearchCV( svm,\n",
        "                        param_distributions=parameter_grid_svm,\n",
        "                        cv=cross_validation,\n",
        "                        scoring='f1')\n",
        "    \n",
        "grid_search_svm.fit(data[features].values, data[target].values)\n",
        "\n",
        "print('Best score of SVM: {}'.format(grid_search_svm.best_score_))\n",
        "print('Best parameters of SVM: {}'.format(grid_search_svm.best_params_))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''parameter_grid_svm_resampled = {'C': [10, 100, 1000],\n",
        "                  'kernel': ['poly', 'rbf'],\n",
        "                  'gamma': [1, 0.1,0.01],\n",
        "                  'coef0': [0.0, 0.1, 0.2],\n",
        "                  }\n",
        "\n",
        "grid_search_svm_resampled = RandomizedSearchCV( svm,\n",
        "                        param_distributions=parameter_grid_svm_resampled,\n",
        "                        cv=cross_validation,\n",
        "                        scoring='accuracy')\n",
        "\n",
        "grid_search_svm_resampled.fit(resampled_data[features].values, resampled_data[target].values)\n",
        "\n",
        "print('Best score of SVM with Resampled Data: {}'.format(grid_search_svm_resampled.best_score_))\n",
        "print('Best parameters of SVM with Resampled Data: {}'.format(grid_search_svm_resampled.best_params_))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''svm_classifier = grid_search_svm.best_estimator_\n",
        "svm_classifier_resampled = grid_search_svm_resampled.best_estimator_'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''# svm\n",
        "num_features_svm = features_num_training_and_predictions_with_KFold(svm_classifier, \"SVM\")\n",
        "print(\"Number of features for SVM: \", num_features_svm)\n",
        "\n",
        "# svm with class weights\n",
        "num_features_svm_resampled = features_num_training_and_predictions_with_KFold(svm_classifier_resampled, \"SVM with Resampled Data\", data, weights_train)\n",
        "print(\"Number of features for SVM with Resampled Data: \", num_features_svm_resampled)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''# get evaluation metrics of SVM\n",
        "model_DT_Kfold, roc_curve_DT = training_and_predictions_with_KFold(svm_classifier, \"SVM\", num_features_svm, data, draw_confusion_matrix=True)\n",
        "\n",
        "# SVM with class weights\n",
        "model_DT_Kfold_resampled , roc_curve_DT_resampled = training_and_predictions_with_KFold(svm_classifier_resampled, \"SVM with Resampled Data\", num_features_svm_resampled, data, draw_confusion_matrix=True)\n",
        "\n",
        "# plot confusion matrix and ROC curve\n",
        "roc_curve_data[\"SVM\"] = roc_curve_DT\n",
        "roc_curve_DT_resampled[\"SVM with Resampled Data\"] = roc_curve_DT_resampled\n",
        "\n",
        "results_training[\"SVM\"] = model_DT_Kfold\n",
        "results_training_resampled[\"SVM with Resampled Data\"] = model_DT_Kfold_resampled\n",
        "\n",
        "# table with model performances\n",
        "model_performances = pd.concat([model_DT_Kfold, model_DT_Kfold_resampled],axis = 0).reset_index()\n",
        "model_performances = model_performances.drop(columns = \"index\",axis =1)\n",
        "model_performances'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Results Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for model_name, info in roc_curve_data.items():\n",
        "    plt.plot(info['fpr'], info['tpr'], label=f'{model_name} (AUC = {info[\"roc_auc\"]:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--') \n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristics')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for model_name, info in pr_curve_data.items():\n",
        "    plt.plot(info['recall'], info['precision'], label=f'{model_name} (AUC = {info[\"precision_recall_auc\"]:.2f})')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision Recall Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "model_performances = pd.concat(results_training.values(), axis=0).reset_index(drop=True)\n",
        "model_performances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### resampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for model_name, info in roc_curve_data_resampled.items():\n",
        "    plt.plot(info['fpr'], info['tpr'], label=f'{model_name} (AUC = {info[\"roc_auc\"]:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--') \n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristics')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for model_name, info in pr_curve_data_resampled.items():\n",
        "    plt.plot(info['recall'], info['precision'], label=f'{model_name} (AUC = {info[\"precision_recall_auc\"]:.2f})')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision Recall Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "model_performances = pd.concat(results_training_resampled.values(), axis=0).reset_index(drop=True)\n",
        "model_performances"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
