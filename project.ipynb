{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8fz9kt5_zLS"
      },
      "source": [
        "# Online Shoppers Purchasing Intention\n",
        "### *Machine Learning Project*, UWr 2024/2025\n",
        "\n",
        "*   Denys Tsebulia 351322,\n",
        "*   Mafalda Costa 351255,\n",
        "*   Mariana Carvalho 351254."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W9htu37_2Bm"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_UabrIg2pQT8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV,KFold, RandomizedSearchCV, ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i2nWmKB-ct4"
      },
      "source": [
        "## **Data analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "GXPYSL7Pop3D",
        "outputId": "0fcdf296-030a-400a-92a5-071f8f628316"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"online_shoppers_intention.csv\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "rfoyI7WPpXY6",
        "outputId": "ccdd9960-73b2-4303-8c0a-8bf22267fb6a"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9jMh1RdpiO8",
        "outputId": "b84d08a8-20d6-4d80-cf8c-de6840536d32"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHLnHy32f9hK"
      },
      "source": [
        "As it can be seen, the majority of the data consists of numerical features. However, there are 8 categorical features: 'Month', 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType', 'Weekend' and 'Revenue'. To utilize these features in our models, we will need to convert them into numerical representations.\n",
        "\n",
        "To **convert 'Revenue', the target feature, into a numerical representation**, we can update the values using the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "V4l2N3te-LQ-"
      },
      "outputs": [],
      "source": [
        "data[\"Revenue\"] = data[\"Revenue\"].apply(lambda x: 1 if x == True else 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUe1aFi_-nj3"
      },
      "source": [
        "### **Separating target, categorical and numerical features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vbjA5ZLS-OF_"
      },
      "outputs": [],
      "source": [
        "target=\"Revenue\"\n",
        "categorical_features=[\"OperatingSystems\", \"Browser\", \"Region\", \"TrafficType\", \"VisitorType\", \"Weekend\", \"Month\"]\n",
        "numerical_features=data.columns.drop(categorical_features).drop(target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0X69PKy-uqR"
      },
      "source": [
        "### **Distribution of 'Revenue'**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9ZFUIyv-wSL"
      },
      "source": [
        "To visualize the distribution of the target feature 'Revenue', we use a count plot. This plot displays the number of occurrences for each class in the 'Revenue' feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "6HGr_feD-sQQ",
        "outputId": "bb677651-d6eb-4ce4-cc32-bc5d41f4cdd4"
      },
      "outputs": [],
      "source": [
        "sb.countplot(x=target,data=data)\n",
        "plt.title(\"Revenue\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5u5z9HF-8Tx"
      },
      "source": [
        "We can observe an imbalance between the two classes of the target feature. The number of instances labeled as 0 ('False') is significantly larger than the number of instances labeled as 1 ('True').\n",
        "This can potentially affect the performance of certain machine learning models, and appropriate techniques such as defining class weights, oversampling and undersampling are required to address this issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8KEpjsJ--Vo"
      },
      "source": [
        "### **NULL values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "66DnvEfk-x1N",
        "outputId": "c8f0e285-c6a4-4ead-ffd7-78d7349f7e06"
      },
      "outputs": [],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH3vU23i_FIk"
      },
      "source": [
        "### **Correlation Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AS2PHa3_G8a"
      },
      "source": [
        "To explore the relationships between all the numerical features, we generate a heatmap:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 995
        },
        "id": "u8yfj2l__Elq",
        "outputId": "ce642436-ca1a-49e1-e3f7-e8fa50029400"
      },
      "outputs": [],
      "source": [
        "temp_data = numerical_features.append(pd.Index([target])) # created Index object to match the numerical features\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "sb.heatmap(data[temp_data].corr(), annot=True, cmap=\"RdBu_r\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IapPXgq_Mp0"
      },
      "source": [
        "Based on the heatmap, the analysis yields the following observations:\n",
        "- There is **generally very little correlation** among the features.\n",
        "- There are a few cases of **high correlation** (|corr| >= 0.7):\n",
        "    - BounceRates & ExitRates (0.9).\n",
        "    - ProductRelated & ProductRelated_Duration (0.86).\n",
        "- There are **moderate correlations** (0.3 < |corr| < 0.7):\n",
        "    - Among the following features: Administrative, Administrative_Duration, Informational, Informational_Duration, ProductRelated, and ProductRelated_Duration.\n",
        "    - And between PageValues and Revenue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsSAyxDE_OfO"
      },
      "source": [
        "To further visualize the relationships between the features, we generate a pairplot for each pair of features, where each session is represented through a purchase (in orange) or no purchase (blue):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YmtjPzwK_A7o",
        "outputId": "748087c3-9329-4bab-cd47-cd49407b2fdf"
      },
      "outputs": [],
      "source": [
        "sb.pairplot(data, hue='Revenue', vars=numerical_features.append(pd.Index([target])), corner=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhfeZock_qUu"
      },
      "source": [
        "From the pairplot, the following observations can be made:\n",
        "- There is no strong correlation between 'Revenue' and any other feature.\n",
        "- Notably, there is a strong negative correlation between PageValues and other features shown in the plot.\n",
        "- We can also see that the there are a few outliers present in the data that may need to be addressed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GvkW8yXHcRo"
      },
      "source": [
        "### **Duplicate rows**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VUizAtq_P69",
        "outputId": "1964fd73-ee7f-462a-9717-58552f9af254"
      },
      "outputs": [],
      "source": [
        "print(\"Total number of duplicate rows: \", data.duplicated().sum())\n",
        "\n",
        "duplicates = data[data.duplicated()]\n",
        "print(\"Duplicate rows:\")\n",
        "print(duplicates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF8ZF_pg_sLD"
      },
      "source": [
        "## **Data pre-processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSpYy9Zz_t8N"
      },
      "source": [
        "### **Remove duplicate rows**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqU67lDLpHWw",
        "outputId": "3719fa7d-6a88-402a-eff4-47454a487128"
      },
      "outputs": [],
      "source": [
        "data.drop_duplicates(inplace=True)\n",
        "print(\"Total number of duplicate rows: \", data.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BoaWqEx_y56"
      },
      "source": [
        "### **Outliers analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqoTLoBS_0EH"
      },
      "source": [
        "Outliers are anomalies within the dataset and are rare occurrences.\n",
        "In this problem, we can think, for example, of a situation where the user accidentally leaves their desktop open on a product page, only to return half an hour later. The website's analytics would register this as the user spending half an hour only reading the product page - a highly unlikely scenario. Such events are not representative of typical user behavior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahv1Vxn3_1ZA"
      },
      "source": [
        "To analyise the outliers, we can look at the pairplot above and the generated boxplots (for each numerical feature, grouped by the 'Revenue' target variable) below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5BM8ywXS_xF_",
        "outputId": "90a06bbd-c865-4748-f8a3-1740e59821bd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i, collumn in enumerate(numerical_features):\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    sb.boxplot(data=data, x=target, y=collumn)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "fhrv1Fi4v-AZ",
        "outputId": "88f136ff-f517-4cdd-dc4f-ef1add3296f5"
      },
      "outputs": [],
      "source": [
        "'''import matplotlib.cbook as cbook\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i, column in enumerate(numerical_features):\n",
        "    stats = {}\n",
        "    stats = cbook.boxplot_stats(data[column], labels=[column])[0]\n",
        "\n",
        "    # set the 2nd and 98th percentiles as the box edges\n",
        "    stats['q1'], stats['q3'] = np.percentile(data[column], [2, 98])\n",
        "    stats['iqr'] = stats['q3'] - stats['q1']  # update IQR\n",
        "\n",
        "    # recompute whiskers\n",
        "    outlier_threshold = 1.5\n",
        "    stats['whislo'] = np.min(data[column][data[column] >= stats['q1'] - outlier_threshold * stats['iqr']])\n",
        "    stats['whishi'] = np.max(data[column][data[column] <= stats['q3'] + outlier_threshold * stats['iqr']])\n",
        "\n",
        "    # update outliers\n",
        "    stats['fliers'] = data[column][(data[column] < stats['whislo']) | (data[column] > stats['whishi'])].tolist()\n",
        "\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.bxp([stats], showfliers=True)\n",
        "    ax.set_title(column)\n",
        "\n",
        "plt.show() '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iP07J0RADql"
      },
      "source": [
        "We can observe that there are outliers present in all features, for both 'Revenue' categories, and that the median and IQR values really vary within the same feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHHjAElRAEya"
      },
      "source": [
        "To remove the outliers, we used the calculation Interquartille Range (IQR) by calculating the percentile for each of the features. We have decided to cut off only very \"far out\" information from the dataset thus including majority of the data, which is between the 2nd\n",
        "percentile and 98th percentile. This is because there is a risk of losing important information if too much data is removed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JT7fxXDu__8A",
        "outputId": "bd7a6b4f-d6c8-4cd7-a462-24833fd6bbf8"
      },
      "outputs": [],
      "source": [
        "Q1 = data[numerical_features].quantile(0.02)\n",
        "Q3 = data[numerical_features].quantile(0.98)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "outlier_threshold = 1.5 # standard threshold\n",
        "\n",
        "# identify outliers for each feature --> create a boolean data frame where a 'True' indicates an outlier in the corresponding feature and row\n",
        "outliers = (data[numerical_features] < (Q1 - outlier_threshold * IQR)) | (data[numerical_features] > (Q3 + outlier_threshold * IQR))\n",
        "\n",
        "# remove outliers from the dataset --> only rows where no features are flagged as outliers are kept\n",
        "data_no_outliers = data[numerical_features][~(outliers.any(axis=1))]\n",
        "\n",
        "# print the number of removed outliers\n",
        "num_outliers_removed = len(data) - len(data_no_outliers)\n",
        "print(\"Number of outliers removed:\", num_outliers_removed)\n",
        "\n",
        "# replace the original data with the no outliers data\n",
        "data[numerical_features] = data_no_outliers\n",
        "\n",
        "# drop rows with missing values --> removing rows with outliers of the numerical columns will leave gaps (NaN) in the other columns\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "for i, collumn in enumerate(numerical_features):\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    sb.boxplot(data=data, x=target, y=collumn)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aEEkC0WAYpz"
      },
      "source": [
        "### **Encode categorical features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwJ_hDZtLKyd"
      },
      "source": [
        "We first deal with the boolean feature 'Weekend', by transforming its value to 1 ('True') and 0 ('False')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-Yb5nqZ0K0CO"
      },
      "outputs": [],
      "source": [
        "data[\"Weekend\"] = data[\"Weekend\"].apply(lambda x: 1 if x == True else 0)\n",
        "categorical_features.remove(\"Weekend\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZtQ-sZsAbtn"
      },
      "source": [
        "To handle the rest of the categorical features in our dataset, we use OneHotEnconder. This transforms the categorical features into numerical representations, and, as a result, the 8 categorical features are expanded into 66 attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "5eTRmyxzAKUi",
        "outputId": "43d714d9-2fc3-42a4-911e-0a6204cafaae"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "for feature in categorical_features:\n",
        "    onehotarray = encoder.fit_transform(data[[feature]]).toarray()\n",
        "    items = [f'{feature}_{item}' for item in encoder.categories_[0]]\n",
        "    data[items] = onehotarray\n",
        "\n",
        "data=data.drop(categorical_features, axis=1)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnyRCjZQK-5D"
      },
      "source": [
        "### **Scaler**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnGll17IAm97"
      },
      "source": [
        "Firstly, we exclude the target feature from the set of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9lYmadvAdDJ",
        "outputId": "3b74ae72-a07b-4a7d-cc3e-113ae708f53c"
      },
      "outputs": [],
      "source": [
        "features=list(data.columns)\n",
        "features.remove('Revenue')\n",
        "features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNgN0ouOAr7l"
      },
      "source": [
        "We apply feature scaling to our subsets, using MinMaxScaler. Feature scaling is particularly beneficial for models that are sensitive to the magnitude of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "-Zek5lcCAoan",
        "outputId": "e561a3db-9f17-40ec-fe13-21ecc298db60"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# (X - X_min) / (X_max - X_min)\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data)\n",
        "scaled_data = pd.DataFrame(scaler.transform(data), columns=data.columns)\n",
        "scaled_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq8CtmDZA-0z"
      },
      "source": [
        "### **Feature selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBRihVr2BAYw"
      },
      "source": [
        "In order to optimize our model's performance and reduce unnecessary complexity, we performed feature selection. With a total of 75 input features, it was essential to identify the features that have the most significant impact on the 'Revenue' and remove those that have a negligible effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRI_CSCzBBoJ"
      },
      "source": [
        "This function performs feature selection using the chi-squared test as the scoring function. It selects the top 'n' features based on their scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG_IYWPEA1Gb",
        "outputId": "1205425d-9cda-4d58-9f18-921748d57525"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "#chi2 (from wikipedia):\n",
        "#In simpler terms, this test is primarily used to examine\n",
        "#whether two categorical variables are independent in influencing the test statistic .\n",
        "\n",
        "def feature_selection(features, n, data):\n",
        "    selector = SelectKBest(score_func=chi2, k=n)\n",
        "    fit=selector.fit(data[features],data[target])\n",
        "\n",
        "    selected_feature_indices = selector.get_support(indices=True)\n",
        "\n",
        "    selected_features = [data[features].columns[i] for i in selected_feature_indices]\n",
        "    return selected_features\n",
        "\n",
        "feature_dict_data = {}\n",
        "for i in range(2, int(len(features))):\n",
        "    feature_dict_data[i] = feature_selection(features, i, data)\n",
        "\n",
        "for key in list(feature_dict_data.keys())[:5]:\n",
        "    print(f\"Key: {key}, Value: {feature_dict_data[key]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNZiXHaeAxYk"
      },
      "source": [
        "### **Resampling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sKnfUTVTI3e",
        "outputId": "544ceec0-abf2-45a0-8af1-0bd9ccfc4c66"
      },
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "rbQkTDVUDJgW",
        "outputId": "39ed4591-b17b-4c3e-92eb-0db927f87e39"
      },
      "outputs": [],
      "source": [
        "data[target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzwLEr1DTL6G"
      },
      "outputs": [],
      "source": [
        "#implement weighted-based resampling\n",
        "class_counts = np.bincount(data[target])\n",
        "class_weights = len(data) / (len(class_counts) * class_counts)\n",
        "weights_train = np.array([class_weights[label] for label in data[target]])\n",
        "weights_train_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "print(\"Class counts:\", class_counts)\n",
        "print(\"Class weights:\", class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resampled_data = data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zqMXl3vqPRM"
      },
      "source": [
        "## **Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40WL0X2Fqa_7"
      },
      "source": [
        "#### k-fold and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "jk_ttsUwVQZ8"
      },
      "outputs": [],
      "source": [
        "def training_and_predictions_with_KFold(Algorithm, name,num_features, data=data, draw_confusion_matrix=False, sample_weight=None):\n",
        "    kf =KFold(n_splits=10)\n",
        "    predictions = []\n",
        "    results = []\n",
        "    prediction_probs = []  # To store prediction probabilities\n",
        "    selected_features=feature_dict_data[num_features]\n",
        "    all_inputs = data[selected_features].values\n",
        "\n",
        "    all_labels = data[target].values\n",
        "\n",
        "    for i, (train_index, test_index) in enumerate(kf.split(all_inputs, all_labels)):\n",
        "        # Split the data into training and testing sets\n",
        "        X_train, X_test = all_inputs[train_index], all_inputs[test_index]\n",
        "        y_train, y_test = all_labels[train_index], all_labels[test_index]\n",
        "        sample_weight_train = sample_weight[train_index] if sample_weight is not None else None\n",
        "        Algorithm.fit(X_train, y_train, sample_weight=sample_weight_train)\n",
        "        predictions.append(Algorithm.predict(X_test))\n",
        "        results.append(y_test)\n",
        "        if(hasattr(Algorithm, \"predict_proba\")):\n",
        "            prediction_probs.append(Algorithm.predict_proba(X_test)[:, 1])\n",
        "\n",
        "    all_predictions = np.concatenate(predictions)\n",
        "    all_results = np.concatenate(results)\n",
        "    if(hasattr(Algorithm, \"predict_proba\")):\n",
        "        all_prediction_probs = np.concatenate(prediction_probs)\n",
        "\n",
        "    #calculates the metrics\n",
        "    accuracy = accuracy_score(all_results, all_predictions)\n",
        "    precision = precision_score(all_results, all_predictions)\n",
        "    recall = recall_score(all_results, all_predictions)\n",
        "    f1 = f1_score(all_results, all_predictions)\n",
        "\n",
        "    #prints the metrics, confusion matrix and ROC curve\n",
        "    if draw_confusion_matrix:\n",
        "        df = pd.DataFrame({\"Model\"           : [name],\n",
        "                       \"Accuracy_score\"  : [accuracy],\n",
        "                       \"Recall_score\"    : [recall],\n",
        "                       \"Precision_score\"       : [precision],\n",
        "                       \"F1_score\"        : [f1],\n",
        "                      })\n",
        "        conf_matrix = confusion_matrix(all_results, all_predictions)\n",
        "        plt.figure(figsize=(10, 5))  # Increase figure size for better visibility\n",
        "        plt.subplot(1, 2, 1)  # First subplot for confusion matrix\n",
        "        sb.heatmap(conf_matrix, cmap='Blues', annot=True, xticklabels=data['Revenue'].unique(), yticklabels=data['Revenue'].unique())\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.title(\"Confusion matrix for \"+name)\n",
        "\n",
        "        # ROC curve\n",
        "        plt.subplot(1, 2, 2)  # Second subplot for ROC curve\n",
        "        fpr, tpr, thresholds = roc_curve(all_results, all_prediction_probs)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver Operating Characteristic for '+name)\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.tight_layout()  # Adjust subplot parameters to give specified padding\n",
        "        plt.show()\n",
        "\n",
        "        # Return both DataFrame and ROC curve metrics\n",
        "        return df, {'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc}\n",
        "    return accuracy, precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "SZR7BvLzWBl5"
      },
      "outputs": [],
      "source": [
        "def features_num_training_and_predictions_with_KFold(Algorithm, name, data=data, sample_weight=None):\n",
        "    accuracy_scores=[]\n",
        "    precision_scores=[]\n",
        "    recall_scores=[]\n",
        "    f1_scores=[]\n",
        "    size_features=[]\n",
        "    max_f1=0\n",
        "    max_precision=0\n",
        "    num_features=0\n",
        "    for i in range(2, int(len(features))):\n",
        "        accuracy_score, precision_score, recall_score, f1_score=training_and_predictions_with_KFold(Algorithm, name, i, data, False, sample_weight)\n",
        "        accuracy_scores.append(accuracy_score)\n",
        "        precision_scores.append(precision_score)\n",
        "        recall_scores.append(recall_score)\n",
        "        f1_scores.append(f1_score)\n",
        "        size_features.append(i)\n",
        "        if(f1_score>max_f1):\n",
        "            max_f1=f1_score\n",
        "            max_precision=precision_score\n",
        "            num_features=i\n",
        "        if(f1_score==max_f1 and precision_score>max_precision):\n",
        "            max_precision=precision_score\n",
        "            num_features=i\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(size_features, accuracy_scores, label=\"accuracy\")\n",
        "    plt.plot(size_features, precision_scores, label=\"precision\")\n",
        "    plt.plot(size_features, recall_scores, label=\"recall\")\n",
        "    plt.plot(size_features, f1_scores, label=\"f1\")\n",
        "    plt.xlabel(\"Number of features\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.title(\"Scores for different number of features of \"+name)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(f\"Features chosen: {feature_dict_data[num_features]}\")\n",
        "    return num_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "o614cPc3WEXC"
      },
      "outputs": [],
      "source": [
        "roc_curve_data = {}\n",
        "roc_curve_data_resampled = {}\n",
        "\n",
        "results_training = {}\n",
        "results_training_resampled ={}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cross_validation = KFold(n_splits=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_classifier = RandomForestClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "grid search on Random Forest (without class weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_parameter_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'n_estimators': [10,100],\n",
        "    'max_depth': [ 4, 5, 6, 9, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
        "    'max_features': ['log2', 2, 4],\n",
        "    'max_leaf_nodes': [None, 2,  5],\n",
        "    'min_samples_leaf': [ 2, 4,],\n",
        "    'min_samples_split': [2],\n",
        "    'class_weight': ['balanced', 'balanced_subsample', None, weights_train_dict],\n",
        "    'random_state': [100],\n",
        "    'ccp_alpha': [ 0.1, 0.2,0.4, 0.5]\n",
        "}\n",
        "\n",
        "random_forest_grid_search = GridSearchCV( random_forest_classifier, param_grid=random_forest_parameter_grid, cv=cross_validation, scoring='f1')\n",
        "random_forest_grid_search.fit(data[features].values, data[target].values)\n",
        "\n",
        "print('Best score for Random Forest: {}'.format(random_forest_grid_search.best_score_))\n",
        "print('Best parameters for Random Forest: {}'.format(random_forest_grid_search.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_parameter_grid_resampled = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'n_estimators': [10,100],\n",
        "    'max_depth': [ 4, 5, 6, 9, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
        "    'max_features': ['log2', 2, 4],\n",
        "    'max_leaf_nodes': [None, 2,  5],\n",
        "    'min_samples_leaf': [ 2, 4,],\n",
        "    'min_samples_split': [2],\n",
        "    'random_state': [100],\n",
        "    'ccp_alpha': [ 0.1, 0.2,0.4, 0.5]\n",
        "}\n",
        "\n",
        "random_forest_grid_search_resampled = GridSearchCV( random_forest_classifier, param_grid=random_forest_parameter_grid_resampled, cv=cross_validation, scoring='precision') \n",
        "random_forest_grid_search_resampled.fit(resampled_data[features].values, resampled_data[target].values)\n",
        "\n",
        "print('Best score for Random Forest: {}'.format(random_forest_grid_search.best_score_))\n",
        "print('Best parameters for Random Forest: {}'.format(random_forest_grid_search.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# random forest classifier\n",
        "random_forest_classifier = random_forest_grid_search.best_estimator_\n",
        "\n",
        "# random forest classifier with resampled data\n",
        "random_forest_classifier_resampled = random_forest_grid_search_resampled.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# retrieving the best number of features for this classifier\n",
        "num_features=features_num_training_and_predictions_with_KFold(random_forest_classifier, \"Random Forest\", data=data)\n",
        "print(\"Number of features Random Forest: \", num_features)\n",
        "\n",
        "num_features_resampled=features_num_training_and_predictions_with_KFold(random_forest_classifier_resampled, \"Random Forest with Resampled Data\", resampled_data)\n",
        "print(\"Number of features Random Forest with Resampled Data: \", num_features_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# random forest\n",
        "# get evaluation metrics and ROC curve\n",
        "model_RF_Kfold, roc_curve_RF = training_and_predictions_with_KFold(random_forest_classifier, \"Random Forest\", 5, data, draw_confusion_matrix=True)\n",
        "\n",
        "# random forest with resampled data\n",
        "model_RF_Kfold_resampled, roc_curve_RF_resampled = training_and_predictions_with_KFold(random_forest_classifier_resampled, \"Random Forest with Resampled Data\", 2 , resampled_data, draw_confusion_matrix=True)\n",
        "\n",
        "# plot confusion matrix and ROC curve\n",
        "roc_curve_data[\"Random Forest\"] = roc_curve_RF\n",
        "roc_curve_data_resampled[\"Random Forest Resampling\"] = roc_curve_RF_resampled\n",
        "\n",
        "results_training[\"Random Forest\"] = model_RF_Kfold\n",
        "results_training_resampled[\"Random Forest Resampling\"] = model_RF_Kfold_resampled\n",
        "\n",
        "# table with model performances\n",
        "model_performances = pd.concat([model_RF_Kfold, model_RF_Kfold_resampled],axis = 0).reset_index()\n",
        "model_performances = model_performances.drop(columns = \"index\",axis =1) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decision_tree_classifier = DecisionTreeClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parameter_grid_decision_tree = {\n",
        "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'min_samples_leaf': [3, 4, 5, 6], \n",
        "    'max_leaf_nodes': [None, 2, 3],\n",
        "    'max_features': [None, 'sqrt', 'log2', 2, 4], \n",
        "    'min_samples_split': [2, 3, 4],\n",
        "    'class_weight': [None, 'balanced', weights_train_dict],\n",
        "    'random_state': [None, 42, 100],\n",
        "    'ccp_alpha': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "      }\n",
        "\n",
        "grid_search_decision_tree = GridSearchCV( decision_tree_classifier,\n",
        "                        param_grid=parameter_grid_decision_tree,\n",
        "                        cv=cross_validation,\n",
        "                        scoring='f1')\n",
        "grid_search_decision_tree.fit(data[features].values, data[target].values)\n",
        "\n",
        "print('Best score of decision tree: {}'.format(grid_search_decision_tree.best_score_))\n",
        "print('Best parameters of decision tree: {}'.format(grid_search_decision_tree.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parameter_grid_decision_tree_resampled = {\n",
        "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'min_samples_leaf': [3, 4, 5, 6], \n",
        "    'max_leaf_nodes': [None, 2, 3],\n",
        "    'max_features': [None, 'sqrt', 'log2', 2, 4], \n",
        "    'min_samples_split': [2, 3, 4],\n",
        "    'random_state': [None, 42, 100],\n",
        "    'ccp_alpha': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "}\n",
        "\n",
        "grid_search_decision_tree_resampled = GridSearchCV( decision_tree_classifier,\n",
        "                        param_grid=parameter_grid_decision_tree_resampled,\n",
        "                        cv=cross_validation,\n",
        "                        scoring='precision')\n",
        "\n",
        "# apply grid search with class weights\n",
        "grid_search_decision_tree_resampled.fit(resampled_data[features].values, resampled_data[target].values)\n",
        "\n",
        "print('Best score of decision tree with class weights: {}'.format(grid_search_decision_tree_resampled.best_score_))\n",
        "print('Best parameters of decision tree with class weights: {}'.format(grid_search_decision_tree_resampled.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "decision_tree_classifier = grid_search_decision_tree.best_estimator_\n",
        "decision_tree_classifier_resampled = grid_search_decision_tree_resampled.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# decision tree\n",
        "num_features_decision_tree = features_num_training_and_predictions_with_KFold(decision_tree_classifier, \"Decision Tree\")\n",
        "print(\"Number of features for Decision Tree: \", num_features_decision_tree)\n",
        "\n",
        "# decision tree with class weights\n",
        "num_features_decision_tree_resampled=features_num_training_and_predictions_with_KFold(decision_tree_classifier_resampled, \"Decision Tree with Resampled Data\", data, weights_train)\n",
        "print(\"Number of features for Decision Tree with Resampled Data: \", num_features_decision_tree_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# decision tree\n",
        "# get evaluation metrics\n",
        "model_DT_Kfold, roc_curve_DT = training_and_predictions_with_KFold(decision_tree_classifier, \"Decision Tree\", 9,  data, draw_confusion_matrix=True)\n",
        "\n",
        "# decision tree with class weights\n",
        "model_DT_Kfold_resampled, roc_curve_DT_resampled = training_and_predictions_with_KFold(num_features_decision_tree_resampled, \"Decision Tree with Resampled Data\", 2, data, draw_confusion_matrix=True)\n",
        "\n",
        "# plot confusion matrix and ROC curve\n",
        "roc_curve_data[\"Decision Tree\"] = roc_curve_DT\n",
        "roc_curve_data_resampled[\"Decision Tree with Resampled Data\"] = roc_curve_DT_resampled\n",
        "\n",
        "results_training[\"Decision Tree\"] = model_DT_Kfold\n",
        "results_training_resampled[\"Decision Tree with Resampled Data\"] = model_DT_Kfold_resampled\n",
        "\n",
        "# table with model performances\n",
        "model_performances = pd.concat([model_DT_Kfold, model_DT_Kfold_resampled],axis = 0).reset_index()\n",
        "model_performances = model_performances.drop(columns = \"index\",axis =1)\n",
        "model_performances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb = XGBClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scale_pos_weight = round(np.bincount(data[target])[0] / np.bincount(data[target])[1])\n",
        "\n",
        "print(f\"scale_pos_weight parameter: {scale_pos_weight}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_parameter_grid = {'n_estimators': [50],\n",
        "                    'max_depth': [3, 5, 10, 50, 100],\n",
        "                    'learning_rate': [0.1],\n",
        "                    'reg_alpha': [0.5],\n",
        "                    'reg_lambda': [0, 0.2, 0.5, 0.8],\n",
        "                    'booster': ['gbtree'],\n",
        "                    'objective': ['binary:logistic'],\n",
        "                    'subsample': [0.5, 0.7, 1.0],\n",
        "                    'colsample_bytree': [0.5, 1.0],\n",
        "                    'gamma': [0, 1],\n",
        "                    'scale_pos_weight' : [1, scale_pos_weight]  #default value is 1, for imbalanced data typical value is ratio of majority class to minority class\n",
        "}\n",
        "\n",
        "xgb_grid_search = GridSearchCV(xgb, param_grid=xgb_parameter_grid, cv=xgb_parameter_grid, scoring='f1')\n",
        "\n",
        "xgb_grid_search.fit(scaled_data[features].values, scaled_data[target].values)\n",
        "\n",
        "print('Best score: {}'.format(xgb_grid_search.best_score_))\n",
        "print('Best parameters: {}'.format(xgb_grid_search.best_params_))\n",
        "svm = SVC()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_parameter_grid_resampled = {'n_estimators': [50],\n",
        "                    'max_depth': [3, 5, 10, 50, 100],\n",
        "                    'learning_rate': [0.1],\n",
        "                    'reg_alpha': [0.5],\n",
        "                    'reg_lambda': [0, 0.2, 0.5, 0.8],\n",
        "                    'booster': ['gbtree'],\n",
        "                    'objective': ['binary:logistic'],\n",
        "                    'subsample': [0.5, 0.7, 1.0],\n",
        "                    'colsample_bytree': [0.5, 1.0],\n",
        "                    'gamma': [0, 1],\n",
        "}\n",
        "\n",
        "xgb_resampled_grid_search = GridSearchCV(xgb, param_grid=xgb_parameter_grid_resampled, cv=cross_validation, scoring='precision')\n",
        "\n",
        "xgb_resampled_grid_search.fit(scaled_resampled_data[features].values, scaled_resampled_data[target].values)\n",
        "\n",
        "print('Best score: {}'.format(xgb_resampled_grid_search.best_score_))\n",
        "print('Best parameters: {}'.format(xgb_resampled_grid_search.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# xgboost classifier\n",
        "xgb_classifier = xgb_grid_search.best_estimator_\n",
        "\n",
        "# xgboost classifier for resampled data\n",
        "xgb_classifier_resampled = xgb_resampled_grid_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best number of features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# retrieving the best number of features for this classifier\n",
        "num_features=features_num_training_and_predictions_with_KFold(xgb_classifier, \"XGBoost\", scaled_data)\n",
        "print(\"Number of features: \", num_features)\n",
        "\n",
        "num_features_resample=features_num_training_and_predictions_with_KFold(xgb_classifier_resampled, \"XGBoost Resampling\", scaled_data_resample)\n",
        "print(\"Number of features resample: \", num_features_resample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performance metrics: confusion matrix, ROC curve, accuracy, recall, precision and F1 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# xgboost\n",
        "# get evaluation metrics and ROC curve\n",
        "xgb_Kfold, roc_curve_XGB=training_and_predictions_with_KFold(xgb_classifier, \"XGBoost\", 41, scaled_data, draw_confusion_matrix=True)\n",
        "\n",
        "# xgboost for resampled data\n",
        "xgb_Kfold_resampling, roc_curve_XGB_resampling=training_and_predictions_with_KFold(xgb_classifier_resampled, \"XGBoost Resampling\", 70, scaled_data_resample, draw_confusion_matrix=True)\n",
        "\n",
        "# plot confusion matrix and ROC curve\n",
        "roc_curve_data[\"XGBoost\"] = roc_curve_XGB\n",
        "roc_curve_data_resampled[\"XGBoost Resampling\"] = roc_curve_XGB_resampling\n",
        "\n",
        "results_training[\"XGBoost\"] = xgb_Kfold\n",
        "results_training_resampled[\"XGBoost Resampling\"] = xgb_Kfold_resampling\n",
        "\n",
        "# table with model performances\n",
        "model_performances = pd.concat([xgb_Kfold, xgb_Kfold_resampling],axis = 0).reset_index()\n",
        "model_performances = model_performances.drop(columns = \"index\",axis =1) \n",
        "model_performances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parameter_grid_svm = {'C': [10, 100, 1000],\n",
        "                  'kernel': ['poly', 'rbf'],\n",
        "                  'gamma': [1, 0.1,0.01],\n",
        "                  'coef0': [0.0, 0.1, 0.2],\n",
        "                  'class_weight': [None, 'balanced', weights_train_dict],\n",
        "                  }\n",
        "\n",
        "\n",
        "grid_search_svm = GridSearchCV( svm,\n",
        "                        param_grid=parameter_grid_svm,\n",
        "                        cv=cross_validation,\n",
        "                        scoring='f1')\n",
        "    \n",
        "grid_search_svm.fit(data[features].values, data[target].values)\n",
        "\n",
        "print('Best score of SVM: {}'.format(grid_search_svm.best_score_))\n",
        "print('Best parameters of SVM: {}'.format(grid_search_svm.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parameter_grid_svm_resampled = {'C': [10, 100, 1000],\n",
        "                  'kernel': ['poly', 'rbf'],\n",
        "                  'gamma': [1, 0.1,0.01],\n",
        "                  'coef0': [0.0, 0.1, 0.2],\n",
        "                  }\n",
        "\n",
        "grid_search_svm_resampled = GridSearchCV( svm,\n",
        "                        param_grid=parameter_grid_svm_resampled,\n",
        "                        cv=cross_validation,\n",
        "                        scoring='precision')\n",
        "\n",
        "grid_search_svm_resampled.fit(resampled_data[features].values, resampled_data[target].values)\n",
        "\n",
        "print('Best score of SVM with Resampled Data: {}'.format(grid_search_svm_resampled.best_score_))\n",
        "print('Best parameters of SVM with Resampled Data: {}'.format(grid_search_svm_resampled.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svm_classifier = grid_search_svm.best_estimator_\n",
        "svm_classifier_resampled = grid_search_svm_resampled.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# svm\n",
        "num_features_svm = features_num_training_and_predictions_with_KFold(svm_classifier, \"SVM\")\n",
        "print(\"Number of features for SVM: \", num_features_svm)\n",
        "\n",
        "# svm with class weights\n",
        "num_features_svm_resampled = features_num_training_and_predictions_with_KFold(svm_classifier_resampled, \"SVM with Resampled Data\", data, weights_train)\n",
        "print(\"Number of features for SVM with Resampled Data: \", num_features_svm_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get evaluation metrics of SVM\n",
        "model_DT_Kfold, roc_curve_DT = training_and_predictions_with_KFold(svm_classifier, \"SVM\", 9,  data, draw_confusion_matrix=True)\n",
        "\n",
        "# SVM with class weights\n",
        "model_DT_Kfold_resampled , roc_curve_DT_resampled = training_and_predictions_with_KFold(svm_classifier_resampled, \"SVM with Resampled Data\", 2, data, draw_confusion_matrix=True)\n",
        "\n",
        "# plot confusion matrix and ROC curve\n",
        "roc_curve_data[\"SVM\"] = roc_curve_DT\n",
        "roc_curve_DT_resampled[\"SVM with Resampled Data\"] = roc_curve_DT_resampled\n",
        "\n",
        "results_training[\"SVM\"] = model_DT_Kfold\n",
        "results_training_resampled[\"SVM with Resampled Data\"] = model_DT_Kfold_resampled\n",
        "\n",
        "# table with model performances\n",
        "model_performances = pd.concat([model_DT_Kfold, model_DT_Kfold_resampled],axis = 0).reset_index()\n",
        "model_performances = model_performances.drop(columns = \"index\",axis =1)\n",
        "model_performances"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
